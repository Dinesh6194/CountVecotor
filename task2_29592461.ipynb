{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>FIT5196- Assignment 1: Parsing Data and Text Preprocessing.</center>\n",
    "## <center>Task 2: Text Preprocessing</center>\n",
    "### Student ID: 29592461\n",
    "### Student Name: Dinesh Karthikeyan\n",
    "#### Language: Python 3.7.0\n",
    "#### Working Environment: Jupyter Notebook\n",
    "#### Packages Used: re, pandas,  numpy, nltk and itertools\n",
    "#### Regex Checker: Pythex.\n",
    "#### Overview of  methodology: \n",
    "* The data set to be parsed is Unit guide (in pdf format) for several units with synopsis and outcome, which is to be preprocessed and converted to count vector file and vocab file.\n",
    "* The *29592461.pdf* document is converted to excel *29592461-converted.xlsx* using `https://smallpdf.com/pdf-to-excel` and the output excel is used to preprocess the text.\n",
    "* First step is to read the contents from the file \"*29592461-converted.xlsx*\".\n",
    "* The file content is loaded into data frame, the synopsis and outcomes are merged together and put in a different column.\n",
    "* Tokenising the words from the column which has merged synopsis and outcomes and storing it in a dictionary based on the unit Id.\n",
    "* The Tokens are used to get the bigrams.\n",
    "* Top 200 bigrams are used based on the pmi measure and frequency filter 5.\n",
    "* After finding the bigrams multi word tokenisation is done and stred in the dictionary.\n",
    "* From this stop words from the file *stopwords_en.txt* is removed.\n",
    "* After removing the stop words the more frequent (95% or 190 units), less frequent (5% or 10 units) and words with length less than three are removed.(based on the document frequency (occurence in diffrenet units.)).\n",
    "* The dictionary with most frequent and less frequent removed is stemmed using porter stemmer.\n",
    "* The words in this dictionary is the final vocabulary and it is written in the file *29592461_vocab.txt* with the index provided for each word (starting from 0).\n",
    "* Now a Count vector file based on the final dictionary of words for units and the vocab file is created and written in the file *29592461_countVec.txt*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Section\n",
    "\n",
    "* re- For Regular Expression functionalities.\n",
    "* numpy- For identifying Nan and removing.\n",
    "* nltk- For all Natural Language tool kit functionalities.\n",
    "* itertools- To convert values in dictionary to list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "from nltk.tokenize import MWETokenizer\n",
    "from nltk.collocations import *\n",
    "from nltk.probability import *\n",
    "import nltk\n",
    "import itertools\n",
    "from itertools import chain\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_file = pd.read_excel('29592461-converted.xlsx') # Loading the converted excel file into dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case Normalisation \n",
    "\n",
    "* Converting the starting letters of the word of the sentences in the synopsis and outcome column.\n",
    "* So as to maintain the Capital Tokens from the mid of the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for each in range(len(read_file)):    \n",
    "    temp_Synopsis=sent_tokenize(read_file.Synopsis[each]) # Sentence tokenising the synopsis column values.\n",
    "    for each_line in range(len(temp_Synopsis)):\n",
    "        # Making the first letter to lower case.\n",
    "        temp_Synopsis[each_line]=temp_Synopsis[each_line][0].lower()+temp_Synopsis[each_line][1:] \n",
    "        temp_Synopsis[each_line]=temp_Synopsis[each_line].replace(\"\\n\",\" \") # Replacing unnecessary new line charaters.\n",
    "        \n",
    "    \n",
    "    read_file.Synopsis[each]=\" \".join(temp_Synopsis) # Grouping the synopsis again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for each in range(len(read_file)):\n",
    "    temp = read_file.Outcomes[each].split(\",\") # Converting the string which is in list format to list using split by \",\".\n",
    "    read_file.Outcomes[each]=\"\".join(temp) # Removing split and converting list to paragraphs.\n",
    "    # Removing unnecessary symbols.\n",
    "    read_file.Outcomes[each] = read_file.Outcomes[each].strip(\"['\") \n",
    "    read_file.Outcomes[each]= read_file.Outcomes[each].strip(\"']\")\n",
    "    read_file.Outcomes[each] = read_file.Outcomes[each].replace(\"'\",'')\n",
    "    read_file.Outcomes[each] = read_file.Outcomes[each].replace('\"','')\n",
    "    read_file.Outcomes[each] = read_file.Outcomes[each].replace(\";\",'. ')\n",
    "    \n",
    "for each in range(len(read_file)):\n",
    "    if read_file.Outcomes[each]!='NA' :\n",
    "        temp_outcome=sent_tokenize(read_file.Outcomes[each]) # Sentence tokenising the Outcomes column values.,\n",
    "        for each_1 in range(len(temp_outcome)):\n",
    "            temp_outcome[each_1]=re.sub(r'\\'|\\\\n',' ',temp_outcome[each_1]).strip() # Replacing unnecessary new line characters. \n",
    "            # Making the first letter to lower case.\n",
    "            temp_outcome[each_1]=temp_outcome[each_1][0].lower()+temp_outcome[each_1][1:]\n",
    "            \n",
    "    \n",
    "        read_file.Outcomes[each]=\" \".join(temp_outcome) # Grouping the Outcomes again.\n",
    "read_file[\"Sy_Out\"]=read_file.Synopsis+\" \"+read_file.Outcomes # Combining thew synopsis and outcomes into a new column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collecting data based on unit\n",
    "* After combining the synopsis and outcomes into a new column a dictionary of thgese value with the unit id as key is created.\n",
    "* If any duplicate unit id is present then its value is appended to the unit id which is already present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_chunk_dict=dict()\n",
    "for each in range(len(read_file.Title)):\n",
    "    if read_file.Title[each] not in unit_chunk_dict.keys():\n",
    "        unit_chunk_dict[read_file.Title[each]]=read_file.Sy_Out[each]\n",
    "    else:\n",
    "        unit_chunk_dict[read_file.Title[each]]+=read_file.Sy_Out[each]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Tokenisation\n",
    "\n",
    "* Creating tokens based on the regex `\\w+(?:[-']\\w+)?` using RegexpTokenizer.\n",
    "* These are the first unigram tokens formed for each unit and are stored in the dictionary based on the unit id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r\"\\w+(?:[-']\\w+)?\")\n",
    "unigram_tokens= dict()\n",
    "for each in unit_chunk_dict.keys():\n",
    "    unigram_tokens[each]=tokenizer.tokenize(unit_chunk_dict[each]) # Tokenising and storing into the dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bigram Formation\n",
    "\n",
    "* Using the unigram tokens formed above, with the help of nltk.collocations bigrams are formed with frequency of minimum 5 units, with the bigram measure of pmi and length of bigram greater than or equal to 3.\n",
    "* Top 200 bigrams are generated and sent MWETokenizer for multi word tokenisation is done and stored into a dictionary based on the unit id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=[]\n",
    "words=list(chain.from_iterable([value for value in unigram_tokens.values()]))\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "bigram_finder = nltk.collocations.BigramCollocationFinder.from_words(words)\n",
    "bigram_finder.apply_freq_filter(5)\n",
    "bigram_finder.apply_word_filter(lambda w: len(w) < 3)\n",
    "top_200_bigrams = bigram_finder.nbest(bigram_measures.pmi, 200) # Top-200 bigrams with pmi measure\n",
    "mwetokenizer = MWETokenizer(top_200_bigrams) \n",
    "# Mwe Tokenisation and storing into dictionary.\n",
    "colloc_values =  dict((uid, mwetokenizer.tokenize(value)) for uid,value in unigram_tokens.items()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop word removal\n",
    "\n",
    "* Removing stop words in the stopwords file *stopwords_en.txt* from the dictionary of unigram and bigram tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_word_file=open(\"stopwords_en.txt\")\n",
    "stop_words=stop_word_file.read().splitlines()\n",
    "stop_words_set=set([each for each in stop_words]) # Stop word set\n",
    "vocab=[]\n",
    "words_1=[]\n",
    "words_1=list(chain.from_iterable(colloc_values.values()))\n",
    "vocab = set(words_1)\n",
    "for each_list in colloc_values.keys():\n",
    "    colloc_values[each_list]=[word for word in colloc_values[each_list] if word not in stop_words_set] # Removing the stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Most Frequent, Less Frequent and Length less than 3 words\n",
    "\n",
    "* With the values the dictionary after removing the stopwords the document frequency is calculated using FreqDist() function.\n",
    "* If the frequency is less than 10 and greater than 190 (5% and 90% for 200 units) the words are removed.\n",
    "* If the length of the word is less than 3 it is removed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_2=list(chain.from_iterable([set(value) for value in colloc_values.values()]))\n",
    "fd_2 = FreqDist(words_2)\n",
    "vocab=set(words_2)\n",
    "for each in colloc_values.keys():\n",
    "     colloc_values[each]=[word for word in colloc_values[each] if word in fd_2.keys() and fd_2[word]>= 10 and fd_2[word] <= 190 and len(word)>=3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming\n",
    "\n",
    "* Porter Stemmer is used to stem the words.\n",
    "* The word in the dictionary after the above step is stemmed to bring it to its root word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps=PorterStemmer()\n",
    "for each in colloc_values.keys():    \n",
    "    temp1=[]\n",
    "    for each_word in colloc_values[each]:\n",
    "        temp1.append(ps.stem(each_word))\n",
    "    colloc_values[each]=temp1   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forming Vocabulary\n",
    "\n",
    "* After stemming, unique words from each unit is added into the list of vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_3=list(chain.from_iterable([set(value) for value in colloc_values.values()]))\n",
    "vocab=set(words_3)\n",
    "final_vocab=dict()\n",
    "vocab=sorted(list(vocab))\n",
    "for index in range(len(vocab)):\n",
    "    final_vocab[index]=vocab[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing into vocabulary file\n",
    "\n",
    "* The final vocabulary list from the above step is written into a vocab file *29592461_vocab.txt*\n",
    "* The file holds the words in the vocabulary in sorted order along with the token index starting from 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = open(\"29592461_vocab.txt\", 'w')\n",
    "for key in final_vocab.keys():\n",
    "    vocab_file.write(final_vocab[key]+\":\"+str(key)+\"\\n\")\n",
    "vocab_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing Count Vector File\n",
    "\n",
    "* Forming a final string to wite into counter vector file.\n",
    "* the count vector file should have *UnitCode,Token_index1:Token1_count_in_this_unit,Token_index2:Token2_count_in_this_unit,.....*\n",
    "* This final string is written into *29592461_countVec.txt*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "final_string=\"\"\n",
    "for unit in colloc_values.keys():\n",
    "    final_string+=unit+\",\"\n",
    "    dict_keys=list(final_vocab.keys())\n",
    "    for token_index in range(len(dict_keys)):\n",
    "        if token_index!=len(dict_keys)-1:\n",
    "            final_string+=str(dict_keys[token_index])+\":\"+str(colloc_values[unit].count(final_vocab[dict_keys[token_index]]))+\",\"\n",
    "        else:\n",
    "            final_string+=str(dict_keys[token_index])+\":\"+str(colloc_values[unit].count(final_vocab[dict_keys[token_index]]))+\"\\n\"\n",
    "count_vector_file=open(\"29592461_countVec.txt\",\"w\")\n",
    "count_vector_file.write(final_string)\n",
    "count_vector_file.close()     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
